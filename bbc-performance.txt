================================================================
(a) MultinomialNB default values, try 1

(b) Confusion matrix
[[85  0  2  0  1]
 [ 1 77  2  0  3]
 [ 3  1 89  0  0]
 [ 0  0  0 91  0]
 [ 0  0  0  0 90]]

(c)                precision    recall  f1-score   support

     business      0.955     0.966     0.960        88
entertainment      0.987     0.928     0.957        83
     politics      0.957     0.957     0.957        93
        sport      1.000     1.000     1.000        91
         tech      0.957     1.000     0.978        90

     accuracy                          0.971       445
    macro avg      0.971     0.970     0.970       445
 weighted avg      0.971     0.971     0.971       445


(d) 
Accuracy of model: 97.08%
Macro-average F1 of model: 97.04%
Weighted-average F1 of model: 97.07%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of world-tokens per class
0. business: 136069
1. entertainment: 96074
2. tech: 145775
3. politics: 132843
4. sport: 152850

(h) Number of world-tokens in entire corpus: 663611

(i) Number and percentage of words with a frequency of zero per class
0. business: 18454 (13.56%)
1. entertainment: 19229 (20.01%)
2. tech: 19327 (13.26%)
3. politics: 19664 (14.80%)
4. sport: 18366 (12.02%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21132 (3.18%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -12.016666049012988
1. entertainment: -11.740021196122866
2. tech: -12.073660626173567
3. politics: -11.996979917438624
4. sport: -11.0146375809022

"Potato":
0. business: -12.016666049012988
1. entertainment: -11.740021196122866
2. tech: -10.687366265053676
3. politics: -11.996979917438624
4. sport: -11.0146375809022
================================================================
================================================================
(a) MultinomialNB default values, try 2

(b) Confusion matrix
[[85  0  2  0  1]
 [ 1 77  2  0  3]
 [ 3  1 89  0  0]
 [ 0  0  0 91  0]
 [ 0  0  0  0 90]]

(c)                precision    recall  f1-score   support

     business      0.955     0.966     0.960        88
entertainment      0.987     0.928     0.957        83
     politics      0.957     0.957     0.957        93
        sport      1.000     1.000     1.000        91
         tech      0.957     1.000     0.978        90

     accuracy                          0.971       445
    macro avg      0.971     0.970     0.970       445
 weighted avg      0.971     0.971     0.971       445


(d) 
Accuracy of model: 97.08%
Macro-average F1 of model: 97.04%
Weighted-average F1 of model: 97.07%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of world-tokens per class
0. business: 136069
1. entertainment: 96074
2. tech: 145775
3. politics: 132843
4. sport: 152850

(h) Number of world-tokens in entire corpus: 663611

(i) Number and percentage of words with a frequency of zero per class
0. business: 18454 (13.56%)
1. entertainment: 19229 (20.01%)
2. tech: 19327 (13.26%)
3. politics: 19664 (14.80%)
4. sport: 18366 (12.02%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21132 (3.18%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -12.016666049012988
1. entertainment: -11.740021196122866
2. tech: -12.073660626173567
3. politics: -11.996979917438624
4. sport: -11.0146375809022

"Potato":
0. business: -12.016666049012988
1. entertainment: -11.740021196122866
2. tech: -10.687366265053676
3. politics: -11.996979917438624
4. sport: -11.0146375809022
================================================================
================================================================
(a) MultinomialNB default values, try 3

(b) Confusion matrix
[[85  0  2  0  1]
 [ 0 79  2  0  2]
 [ 5  1 87  0  0]
 [ 0  0  0 91  0]
 [ 0  1  0  0 89]]

(c)                precision    recall  f1-score   support

     business      0.944     0.966     0.955        88
entertainment      0.975     0.952     0.963        83
     politics      0.956     0.935     0.946        93
        sport      1.000     1.000     1.000        91
         tech      0.967     0.989     0.978        90

     accuracy                          0.969       445
    macro avg      0.969     0.968     0.968       445
 weighted avg      0.969     0.969     0.968       445


(d) 
Accuracy of model: 96.85%
Macro-average F1 of model: 96.84%
Weighted-average F1 of model: 96.85%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of world-tokens per class
0. business: 136069
1. entertainment: 96074
2. tech: 145775
3. politics: 132843
4. sport: 152850

(h) Number of world-tokens in entire corpus: 663611

(i) Number and percentage of words with a frequency of zero per class
0. business: 18454 (13.56%)
1. entertainment: 19229 (20.01%)
2. tech: 19327 (13.26%)
3. politics: 19664 (14.80%)
4. sport: 18366 (12.02%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21132 (3.18%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -21.031279382859992
1. entertainment: -20.68324500162074
2. tech: -21.100180170335864
3. politics: -21.007285777722096
4. sport: -11.244034396106377

"Potato":
0. business: -21.031279382859992
1. entertainment: -20.68324500162074
2. tech: -10.791194176913782
3. politics: -21.007285777722096
4. sport: -11.244034396106377
================================================================
================================================================
(a) MultinomialNB default values, try 4

(b) Confusion matrix
[[85  0  2  0  1]
 [ 1 77  2  0  3]
 [ 3  1 89  0  0]
 [ 0  0  0 91  0]
 [ 0  0  0  0 90]]

(c)                precision    recall  f1-score   support

     business      0.955     0.966     0.960        88
entertainment      0.987     0.928     0.957        83
     politics      0.957     0.957     0.957        93
        sport      1.000     1.000     1.000        91
         tech      0.957     1.000     0.978        90

     accuracy                          0.971       445
    macro avg      0.971     0.970     0.970       445
 weighted avg      0.971     0.971     0.971       445


(d) 
Accuracy of model: 97.08%
Macro-average F1 of model: 97.04%
Weighted-average F1 of model: 97.07%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of world-tokens per class
0. business: 136069
1. entertainment: 96074
2. tech: 145775
3. politics: 132843
4. sport: 152850

(h) Number of world-tokens in entire corpus: 663611

(i) Number and percentage of words with a frequency of zero per class
0. business: 18454 (13.56%)
1. entertainment: 19229 (20.01%)
2. tech: 19327 (13.26%)
3. politics: 19664 (14.80%)
4. sport: 18366 (12.02%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21132 (3.18%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -12.104088522204611
1. entertainment: -11.821658568144791
2. tech: -12.162085345786851
3. politics: -12.084042478566454
4. sport: -11.03226609198822

"Potato":
0. business: -12.104088522204611
1. entertainment: -11.821658568144791
2. tech: -10.695748276993424
3. politics: -12.084042478566454
4. sport: -11.03226609198822
================================================================
