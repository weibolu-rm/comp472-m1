================================================================
(a) MultinomialNB default values, try 1

(b) Confusion matrix
[[104   0   2   0   3]
 [  0  76   4   0   2]
 [  0   0  76   0   0]
 [  0   0   0 100   0]
 [  0   0   0   0  78]]

(c)                precision    recall  f1-score   support

     business      1.000     0.954     0.977       109
entertainment      1.000     0.927     0.962        82
     politics      0.927     1.000     0.962        76
        sport      1.000     1.000     1.000       100
         tech      0.940     1.000     0.969        78

     accuracy                          0.975       445
    macro avg      0.973     0.976     0.974       445
 weighted avg      0.977     0.975     0.975       445


(d) 
Accuracy of model: 97.53%
Macro-average F1 of model: 97.39%
Weighted-average F1 of model: 97.53%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of word-tokens per class
0. business: 129781
1. entertainment: 95002
2. tech: 152412
3. politics: 132138
4. sport: 161394

(h) Number of word-tokens in entire corpus: 670727

(i) Number and percentage of words with a frequency of zero per class
0. business: 18864 (14.54%)
1. entertainment: 19161 (20.17%)
2. tech: 19061 (12.51%)
3. politics: 19715 (14.92%)
4. sport: 18174 (11.26%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21009 (3.13%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -11.977929115127578
1. entertainment: -10.122004417223344
2. tech: -12.110843962404905
3. politics: -11.99262568000729
4. sport: -11.060447362590311

"Potato":
0. business: -11.977929115127578
1. entertainment: -11.731442329657444
2. tech: -12.110843962404905
3. politics: -11.99262568000729
4. sport: -10.77276529013853
================================================================
================================================================
(a) MultinomialNB default values, try 2

(b) Confusion matrix
[[104   0   2   0   3]
 [  0  76   4   0   2]
 [  0   0  76   0   0]
 [  0   0   0 100   0]
 [  0   0   0   0  78]]

(c)                precision    recall  f1-score   support

     business      1.000     0.954     0.977       109
entertainment      1.000     0.927     0.962        82
     politics      0.927     1.000     0.962        76
        sport      1.000     1.000     1.000       100
         tech      0.940     1.000     0.969        78

     accuracy                          0.975       445
    macro avg      0.973     0.976     0.974       445
 weighted avg      0.977     0.975     0.975       445


(d) 
Accuracy of model: 97.53%
Macro-average F1 of model: 97.39%
Weighted-average F1 of model: 97.53%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of word-tokens per class
0. business: 129781
1. entertainment: 95002
2. tech: 152412
3. politics: 132138
4. sport: 161394

(h) Number of word-tokens in entire corpus: 670727

(i) Number and percentage of words with a frequency of zero per class
0. business: 18864 (14.54%)
1. entertainment: 19161 (20.17%)
2. tech: 19061 (12.51%)
3. politics: 19715 (14.92%)
4. sport: 18174 (11.26%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21009 (3.13%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -11.977929115127578
1. entertainment: -10.122004417223344
2. tech: -12.110843962404905
3. politics: -11.99262568000729
4. sport: -11.060447362590311

"Potato":
0. business: -11.977929115127578
1. entertainment: -11.731442329657444
2. tech: -12.110843962404905
3. politics: -11.99262568000729
4. sport: -10.77276529013853
================================================================
================================================================
(a) MultinomialNB smoothing=0.0001

(b) Confusion matrix
[[103   2   1   0   3]
 [  0  79   3   0   0]
 [  1   0  75   0   0]
 [  0   0   0 100   0]
 [  0   0   0   0  78]]

(c)                precision    recall  f1-score   support

     business      0.990     0.945     0.967       109
entertainment      0.975     0.963     0.969        82
     politics      0.949     0.987     0.968        76
        sport      1.000     1.000     1.000       100
         tech      0.963     1.000     0.981        78

     accuracy                          0.978       445
    macro avg      0.976     0.979     0.977       445
 weighted avg      0.978     0.978     0.977       445


(d) 
Accuracy of model: 97.75%
Macro-average F1 of model: 97.71%
Weighted-average F1 of model: 97.75%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of word-tokens per class
0. business: 129781
1. entertainment: 95002
2. tech: 152412
3. politics: 132138
4. sport: 161394

(h) Number of word-tokens in entire corpus: 670727

(i) Number and percentage of words with a frequency of zero per class
0. business: 18864 (14.54%)
1. entertainment: 19161 (20.17%)
2. tech: 19061 (12.51%)
3. politics: 19715 (14.92%)
4. sport: 18174 (11.26%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21009 (3.13%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -20.983966734940445
1. entertainment: -10.075364830527441
2. tech: -21.144702334680332
3. politics: -21.0019647470942
4. sport: -11.298424909231864

"Potato":
0. business: -20.983966734940445
1. entertainment: -20.67202456331102
2. tech: -21.144702334680332
3. politics: -21.0019647470942
4. sport: -10.89297646709595
================================================================
================================================================
(a) MultinomialNB smoothing=0.9

(b) Confusion matrix
[[104   0   2   0   3]
 [  0  76   4   0   2]
 [  0   0  76   0   0]
 [  0   0   0 100   0]
 [  0   0   0   0  78]]

(c)                precision    recall  f1-score   support

     business      1.000     0.954     0.977       109
entertainment      1.000     0.927     0.962        82
     politics      0.927     1.000     0.962        76
        sport      1.000     1.000     1.000       100
         tech      0.940     1.000     0.969        78

     accuracy                          0.975       445
    macro avg      0.973     0.976     0.974       445
 weighted avg      0.977     0.975     0.975       445


(d) 
Accuracy of model: 97.53%
Macro-average F1 of model: 97.39%
Weighted-average F1 of model: 97.53%

(e) Priors
business: 22.92%
entertainment: 17.35%
tech: 18.02%
politics: 18.74%
sport: 22.97%

(f) Size of vocabulary: 29421

(g) Number of word-tokens per class
0. business: 129781
1. entertainment: 95002
2. tech: 152412
3. politics: 132138
4. sport: 161394

(h) Number of word-tokens in entire corpus: 670727

(i) Number and percentage of words with a frequency of zero per class
0. business: 18864 (14.54%)
1. entertainment: 19161 (20.17%)
2. tech: 19061 (12.51%)
3. politics: 19715 (14.92%)
4. sport: 18174 (11.26%)

(j) Number and percentage of words with a frequency of one in entire corpus:
21009 (3.13%)

(k) Two favorite words in vocabulary, and their log-prob
"Zombie":
0. business: -12.064636441249315
1. entertainment: -10.118277122642596
2. tech: -12.199891917455085
3. politics: -12.079607655540928
4. sport: -11.078810212355666

"Potato":
0. business: -12.064636441249315
1. entertainment: -11.812872843417002
2. tech: -12.199891917455085
3. politics: -12.079607655540928
4. sport: -10.782544396212494
================================================================
